{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append(\"../lib\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading the model\n",
    "\n",
    "We prepared our custom Steam dataset with over 80,000 games and corresponding images for each of them. We can read this dataset from the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>image</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>553850</td>\n",
       "      <td>HELLDIVERS™ 2</td>\n",
       "      <td>images/553850/ss_0c79f56fc7be1bd0102f2ca1c92c8...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>553850</td>\n",
       "      <td>HELLDIVERS™ 2</td>\n",
       "      <td>images/553850/ss_33e684e9cb2517af1599f0ca2b57d...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>553850</td>\n",
       "      <td>HELLDIVERS™ 2</td>\n",
       "      <td>images/553850/ss_8949ed7dd24a02d5ea13b08fc5c04...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>553850</td>\n",
       "      <td>HELLDIVERS™ 2</td>\n",
       "      <td>images/553850/ss_50afbbc4d811c38fe9f64c1fc8d7e...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>553850</td>\n",
       "      <td>HELLDIVERS™ 2</td>\n",
       "      <td>images/553850/ss_cb276fe9f0b09683bdbc496f82b40...</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          title                                              image  \\\n",
       "0  553850  HELLDIVERS™ 2  images/553850/ss_0c79f56fc7be1bd0102f2ca1c92c8...   \n",
       "1  553850  HELLDIVERS™ 2  images/553850/ss_33e684e9cb2517af1599f0ca2b57d...   \n",
       "2  553850  HELLDIVERS™ 2  images/553850/ss_8949ed7dd24a02d5ea13b08fc5c04...   \n",
       "3  553850  HELLDIVERS™ 2  images/553850/ss_50afbbc4d811c38fe9f64c1fc8d7e...   \n",
       "4  553850  HELLDIVERS™ 2  images/553850/ss_cb276fe9f0b09683bdbc496f82b40...   \n",
       "\n",
       "   genres  \n",
       "0  Action  \n",
       "1  Action  \n",
       "2  Action  \n",
       "3  Action  \n",
       "4  Action  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import msgspec\n",
    "from steam_model import SteamGame\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = Path(\"./data/steam\")\n",
    "steam_games = data_root / \"games.json\"\n",
    "\n",
    "games = msgspec.json.decode(steam_games.read_text(), type=list[SteamGame])\n",
    "\n",
    "excluded_genres = set([\n",
    "    \"Animation & Modeling\",\n",
    "    \"Audio Production\",\n",
    "    \"Design & Illustration\",\n",
    "    \"Photo Editing\",\n",
    "    \"Software Training\",\n",
    "    \"Utilities\",\n",
    "    \"Video Production\",\n",
    "    \"Web Publishing\"\n",
    "])\n",
    "\n",
    "dataset = [\n",
    "    {\n",
    "        \"id\": game.id,\n",
    "        \"title\": game.page_information.title,\n",
    "        \"image\": image,\n",
    "        \"genres\": \",\".join(game.page_information.genres)\n",
    "    }\n",
    "    for game\n",
    "    in games[:10000]\n",
    "    if len(game.page_information.images) > 0\n",
    "        and game.popularity_rank < 5000\n",
    "        and len(game.page_information.genres) > 0\n",
    "        and next((genre for genre in game.page_information.genres if genre in excluded_genres), None) is None\n",
    "    for image in game.page_information.images\n",
    "]\n",
    "df = pd.DataFrame.from_records(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a few convenience functions to easily get an image for a given game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model_id = \"openai/clip-vit-large-patch14\"\n",
    "processor_model_id = model_id\n",
    "\n",
    "processor = typing.cast(CLIPProcessor, CLIPProcessor.from_pretrained(processor_model_id))\n",
    "model = typing.cast(CLIPModel, CLIPModel.from_pretrained(model_id))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "from typing import TypedDict\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SteamGenreItem(TypedDict):\n",
    "    image: PIL.Image.Image\n",
    "    genres: str\n",
    "\n",
    "class SteamGenresDataset(Dataset):\n",
    "    def __init__(self, root: Path, dataset: pd.DataFrame, processor: CLIPProcessor):\n",
    "        self.root = root\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> SteamGenreItem:\n",
    "        item = self.dataset.iloc[idx]\n",
    "        image_path = self.root / item[\"image\"]\n",
    "        image = PIL.Image.open(image_path)\n",
    "        genres = item[\"genres\"]\n",
    "\n",
    "        return { \"image\": image, \"genres\": genres, }\n",
    "\n",
    "def collate_fn(batch: list[SteamGenreItem]):\n",
    "    def toinp(text: str) -> torch.Tensor:\n",
    "        input_ids = processor.tokenizer(text, padding=\"max_length\", truncation=True).input_ids\n",
    "        return input_ids\n",
    "\n",
    "    def topix(image: PIL.Image.Image) -> torch.Tensor:\n",
    "        pixel_values = processor.image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "        return pixel_values.squeeze()\n",
    "\n",
    "    return {\n",
    "        'pixel_values': torch.stack([topix(x[\"image\"]) for x in batch]),\n",
    "        'input_ids': torch.tensor([toinp(x[\"genres\"]) for x in batch]),\n",
    "        \"return_loss\": True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our data into training and validation sets\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and verify that the dataset works as we expect\n",
    "train_dataset = SteamGenresDataset(root=data_root, dataset=train_df, processor=processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1)\n",
    "eval_dataset = SteamGenresDataset(root=data_root, dataset=eval_df, processor=processor)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-1.7631, -1.7923, -1.7923,  ..., -1.6171, -1.6171, -1.6317],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.6171, -1.6171, -1.6317],\n",
       "           [-1.7923, -1.7923, -1.7923,  ..., -1.6171, -1.6171, -1.6317],\n",
       "           ...,\n",
       "           [-1.7631, -1.7631, -1.7485,  ..., -1.7193, -1.7193, -1.7193],\n",
       "           [-1.7631, -1.7777, -1.7777,  ..., -1.7193, -1.7193, -1.7193],\n",
       "           [-1.7339, -1.7777, -1.7777,  ..., -1.7339, -1.7339, -1.7193]],\n",
       " \n",
       "          [[-1.1668, -1.1818, -1.1668,  ..., -1.3769, -1.3769, -1.3919],\n",
       "           [-1.1818, -1.1518, -1.1368,  ..., -1.3769, -1.3769, -1.3919],\n",
       "           [-1.1668, -1.1218, -1.1518,  ..., -1.3769, -1.3769, -1.3919],\n",
       "           ...,\n",
       "           [ 0.1539,  0.2890,  0.2589,  ..., -1.6621, -1.6621, -1.6621],\n",
       "           [ 0.1989,  0.3340,  0.3190,  ..., -1.6621, -1.6621, -1.6621],\n",
       "           [ 0.1839,  0.2740,  0.2439,  ..., -1.6771, -1.6771, -1.6621]],\n",
       " \n",
       "          [[-0.5559, -0.5275, -0.5133,  ..., -1.0110, -1.0110, -1.0252],\n",
       "           [-0.5701, -0.5275, -0.5133,  ..., -1.0110, -1.0110, -1.0252],\n",
       "           [-0.5701, -0.5275, -0.5275,  ..., -1.0110, -1.0110, -1.0252],\n",
       "           ...,\n",
       "           [ 1.0510,  1.2358,  1.2785,  ..., -1.3380, -1.3380, -1.3380],\n",
       "           [ 1.1078,  1.3354,  1.2785,  ..., -1.3380, -1.3380, -1.3380],\n",
       "           [ 1.1363,  1.2216,  1.1647,  ..., -1.3665, -1.3807, -1.3522]]]]),\n",
       " 'input_ids': tensor([[49406,  1816,   267,  2090,  3822, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "          49407, 49407, 49407, 49407, 49407, 49407, 49407]]),\n",
       " 'return_loss': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn([train_dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, IntervalStrategy\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"steam_genre_classifier\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    save_strategy=IntervalStrategy.STEPS,\n",
    "    save_total_limit=10,\n",
    "    eval_steps=250,\n",
    "    save_steps=250,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    # Comment these out when testing new models to check for CUDA OOM errors quicker, but for debugging, it just makes things more difficult\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"reduce-overhead\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor.image_processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
